{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drake/anaconda3/envs/.torch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/drake/anaconda3/envs/.torch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/drake/anaconda3/envs/.torch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/drake/anaconda3/envs/.torch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/drake/anaconda3/envs/.torch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/drake/anaconda3/envs/.torch/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from Models.selective_sequential import *\n",
    "from Loss.triplet_regularized import *\n",
    "from session import *\n",
    "from LR_Schedule.cyclical import Cyclical\n",
    "from LR_Schedule.cos_anneal import CosAnneal\n",
    "from LR_Schedule.lr_find import lr_find\n",
    "from callbacks import *\n",
    "from validation import *\n",
    "import Datasets.ImageData as ImageData\n",
    "from Transforms.ImageTransforms import *\n",
    "import util\n",
    "from session import LossMeter, EvalModel\n",
    "from Layers.flatten import Flatten\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drake/anaconda3/envs/.torch/lib/python3.7/site-packages/torch/cuda/__init__.py:135: UserWarning: \n",
      "    Found GPU0 GeForce GTX 770 which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability that we support is 3.5.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.cuda.set_device(0); torch.backends.cudnn.benchmark=True;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = datasets.MNIST('/media/drake/MX500/Datasets/mnist/train', download=True, train=True, transform=transform)\n",
    "partial_trainset = torch.utils.data.dataset.Subset(trainset, np.arange(500))\n",
    "\n",
    "valset = datasets.MNIST('/media/drake/MX500/Datasets/mnist/test', download=True, train=False, transform=transform)\n",
    "partial_valset = torch.utils.data.dataset.Subset(valset, np.arange(500))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(partial_trainset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(partial_valset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = ['max1', 'act1', 'out']\n",
    "\n",
    "def make_model():\n",
    "    return SelectiveSequential(\n",
    "    select,\n",
    "    {'conv64': nn.Conv2d(1, 64, kernel_size=5, padding=2),\n",
    "     'act64': nn.ReLU(True),\n",
    "     \n",
    "     'max1': nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "     'conv192': nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "     'act192': nn.ReLU(True),\n",
    "    \n",
    "     'max2': nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "     'conv384': nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "     'act384': nn.ReLU(True),\n",
    "     \n",
    "     'conv256a': nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "     'act256a': nn.ReLU(True),\n",
    "     \n",
    "     'conv256b': nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "     'act256b': nn.ReLU(True),\n",
    "     \n",
    "     'max3': nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "     'flatten': Flatten(),\n",
    "     'fc1': nn.Linear(3 * 3 * 256, 512),\n",
    "     'act1': nn.ReLU(True),\n",
    "     'fc2': nn.Linear(512, 512),\n",
    "     'act2': nn.ReLU(True),\n",
    "     'out': nn.Linear(512, 10)})\n",
    "\n",
    "model = make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = TripletRegularizedMultiMarginLoss(.5, .5, select)\n",
    "sess = Session(model, criterion, optim.Adam, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "validator = EmbeddingSpaceValidator(valloader, select, CustomOneHotAccuracy, \n",
    "                                    model_file=\"./triplet-reg.ckpt.tar\")\n",
    "lr_scheduler = CosAnneal(len(trainloader)*50, T_mult=1, lr_min=1e-6)\n",
    "schedule = TrainingSchedule(trainloader, 50, [lr_scheduler, validator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LOADING CHECKPOINT ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f32cee5bc64448915f24372f905b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epochs', max=50, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Steps', max=8, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Validating', max=8, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val accuracy:  0.712 \n",
      "train loss:  1.8084  train cross entropy loss :  0.1756 \n",
      "valid loss:  2.0587  valid cross entropy loss :  0.2509\n",
      "--- CHECKPOINT ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Steps', max=8, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Validating', max=8, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val accuracy:  0.83 \n",
      "train loss:  1.7935  train cross entropy loss :  0.1697 \n",
      "valid loss:  1.9993  valid cross entropy loss :  0.2069\n",
      "--- CHECKPOINT ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Steps', max=8, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Validating', max=8, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val accuracy:  0.868 \n",
      "train loss:  1.7565  train cross entropy loss :  0.1299 \n",
      "valid loss:  1.9405  valid cross entropy loss :  0.1899\n",
      "--- CHECKPOINT ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Steps', max=8, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Validating', max=8, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val accuracy:  0.872 \n",
      "train loss:  1.6988  train cross entropy loss :  0.0977 \n",
      "valid loss:  1.9086  valid cross entropy loss :  0.1476\n",
      "--- CHECKPOINT ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b754861b7a45fdb5ce3cce6454924e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Steps', max=8, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess.train(schedule, \"MNIST-TripletReg.ckpt.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.schedule.callbacks[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.load(\"./triplet-reg.ckpt.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=False)\n",
    "total_validator = EmbeddingSpaceValidator(total_valloader, [], CustomOneHotAccuracy)\n",
    "\n",
    "total_validator.run(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(total_validator.val_accuracies), \"Best accuracy without reg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lmbda, train_loader, val_loader):\n",
    "    print(f\"Training: {lmbda}\")\n",
    "    num_epochs = 20\n",
    "    validator = EmbeddingSpaceValidator(val_loader, [], CustomOneHotAccuracy)\n",
    "    lr_scheduler = CosAnneal(len(train_loader)*num_epochs, T_mult=1, lr_min=1e-6)\n",
    "    schedule = TrainingSchedule(train_loader, [lr_scheduler, validator])\n",
    "    criterion = TripletRegularizedMultiMarginLoss(lmbda, .5, select)\n",
    "    sess = Session(make_model(), criterion, optim.Adam, 1e-4)\n",
    "    sess.train(schedule, num_epochs)\n",
    "    return np.max(validator.val_accuracies)\n",
    "\n",
    "losses = {}\n",
    "\n",
    "def search(lower, middle, upper, max_interval = .01, lower_acc=None, upper_acc=None):\n",
    "    if (upper-lower < max_interval): return\n",
    "\n",
    "    if lower_acc == None: \n",
    "        lower_acc = train(lower, trainloader, valloader)\n",
    "        losses[lower] = lower_acc\n",
    "    \n",
    "    middle_acc = train(middle, trainloader, valloader)\n",
    "    losses[middle] = middle_acc\n",
    "    \n",
    "    if upper_acc == None: \n",
    "        upper_acc = train(upper, trainloader, valloader)\n",
    "        losses[upper] = upper_acc\n",
    "        \n",
    "    lower_mean = (lower_acc + middle_acc) / 2\n",
    "    upper_mean = (upper_acc + middle_acc) / 2\n",
    "    \n",
    "    if lower_acc > upper_acc:\n",
    "        search(lower, (middle-lower)/2, middle, max_interval, lower_acc, middle_acc)\n",
    "    else:\n",
    "        search(middle, (upper-middle)/2, upper, max_interval, middle_acc, upper_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search(0, .5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_set = torch.utils.data.dataset.Subset(valset, np.arange(500))\n",
    "dataloader = torch.utils.data.DataLoader(visualization_set, batch_size=64, shuffle=False)\n",
    "\n",
    "tensorboard_embeddings(model, ['max1'], \n",
    "                       dataloader, \n",
    "                       valset.targets[:500], \n",
    "                       1.0 - valset.data[:500].reshape(-1, 1, 28, 28) / 255.0, \n",
    "                       './mnist_tripletreg')\n",
    "\n",
    "tensorboard_embeddings(model, ['max2'], \n",
    "                       dataloader, \n",
    "                       valset.targets[:500], \n",
    "                       1.0 - valset.data[:500].reshape(-1, 1, 28, 28) / 255.0, \n",
    "                       './mnist_tripletreg')\n",
    "\n",
    "tensorboard_embeddings(model, ['max3'], \n",
    "                       dataloader, \n",
    "                       valset.targets[:500], \n",
    "                       1.0 - valset.data[:500].reshape(-1, 1, 28, 28) / 255.0, \n",
    "                       './mnist_tripletreg')\n",
    "\n",
    "tensorboard_embeddings(model, ['act1'], \n",
    "                       dataloader, \n",
    "                       valset.targets[:500], \n",
    "                       1.0 - valset.data[:500].reshape(-1, 1, 28, 28) / 255.0, \n",
    "                       './mnist_tripletreg')\n",
    "\n",
    "tensorboard_embeddings(model, ['act2'], \n",
    "                       dataloader, \n",
    "                       valset.targets[:500], \n",
    "                       1.0 - valset.data[:500].reshape(-1, 1, 28, 28) / 255.0, \n",
    "                       './mnist_tripletreg')\n",
    "\n",
    "tensorboard_embeddings(model, ['out'], \n",
    "                       dataloader, \n",
    "                       valset.targets[:500], \n",
    "                       1.0 - valset.data[:500].reshape(-1, 1, 28, 28) / 255.0, \n",
    "                       './mnist_tripletreg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
